{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Ollama Testing Notebook\n",
    "\n",
    "This notebook demonstrates various methods of interacting with Ollama, including:\n",
    "- Different invocation methods (native Python, LangChain, raw REST)\n",
    "- Streaming vs non-streaming responses\n",
    "- Metrics collection (token counts, response times, durations)\n",
    "- Model management and listing\n",
    "- Performance comparisons\n",
    "\n",
    "**Prerequisites:**\n",
    "- Ollama running locally on `http://localhost:11434`\n",
    "- Model `gemma3n:e4b` pulled and available\n",
    "- Python packages: `ollama`, `langchain-community`, `langchain-core`, `pandas`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python Environment Information:\n",
      "   Python Version: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "   Python Executable: d:\\Dev\\studio\\studio-backend\\.venv\\Scripts\\python.exe\n",
      "   Python Path: C:\\Python313\\python313.zip\n",
      "   Virtual Environment: d:\\Dev\\studio\\studio-backend\\.venv\n",
      "   Environment Name: .venv\n",
      "   Working Directory: d:\\Dev\\studio\\studio-backend\\inference\n",
      "\n",
      "üì¶ Package Availability Check:\n",
      "   ‚úÖ ollama: Available\n",
      "   ‚úÖ langchain: Available\n",
      "   ‚úÖ langchain_community: Available\n",
      "   ‚úÖ pandas: Available\n",
      "   ‚úÖ requests: Available\n"
     ]
    }
   ],
   "source": [
    "# Check Python environment and version\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üêç Python Environment Information:\")\n",
    "print(f\"   Python Version: {sys.version}\")\n",
    "print(f\"   Python Executable: {sys.executable}\")\n",
    "print(f\"   Python Path: {sys.path[0]}\")\n",
    "\n",
    "# Check if we're in a virtual environment\n",
    "if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):\n",
    "    print(f\"   Virtual Environment: {sys.prefix}\")\n",
    "    venv_name = Path(sys.prefix).name\n",
    "    print(f\"   Environment Name: {venv_name}\")\n",
    "else:\n",
    "    print(\"   Virtual Environment: Not detected (using system Python)\")\n",
    "\n",
    "# Check current working directory\n",
    "print(f\"   Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if common packages are available\n",
    "packages_to_check = ['ollama', 'langchain', 'langchain_community', 'pandas', 'requests']\n",
    "print(f\"\\nüì¶ Package Availability Check:\")\n",
    "for package in packages_to_check:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"   ‚úÖ {package}: Available\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå {package}: Not installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration:\n",
      "   Ollama Host: http://localhost:11434\n",
      "   Model: gemma3n:e4b\n",
      "\n",
      "‚úÖ Ollama connection successful\n",
      "   Version: 0.9.3\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Environment Setup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "MODEL_NAME = os.getenv(\"OLLAMA_MODEL\", \"gemma3n:e4b\")\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"   Ollama Host: {BASE_URL}\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "\n",
    "# Helper function to display metrics in a readable format\n",
    "def show_metrics(response_data: dict, title: str = \"Metrics\"):\n",
    "    \"\"\"Extract and display timing/token metrics from Ollama response\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Extract timing metrics (convert from nanoseconds to milliseconds)\n",
    "    for key in [\"total_duration\", \"load_duration\", \"prompt_eval_duration\", \"eval_duration\"]:\n",
    "        if key in response_data:\n",
    "            metrics[key.replace(\"_duration\", \"_ms\")] = round(response_data[key] / 1e6, 2)\n",
    "    \n",
    "    # Extract token counts\n",
    "    for key in [\"prompt_eval_count\", \"eval_count\"]:\n",
    "        if key in response_data:\n",
    "            metrics[key] = response_data[key]\n",
    "    \n",
    "    # Calculate tokens per second if we have the data\n",
    "    if \"eval_count\" in response_data and \"eval_duration\" in response_data and response_data[\"eval_duration\"] > 0:\n",
    "        tokens_per_sec = response_data[\"eval_count\"] / (response_data[\"eval_duration\"] / 1e9)\n",
    "        metrics[\"tokens_per_second\"] = round(tokens_per_sec, 2)\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  {title}\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Test connection to Ollama\n",
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/api/version\", timeout=5)\n",
    "    print(f\"\\n‚úÖ Ollama connection successful\")\n",
    "    print(f\"   Version: {response.json().get('version', 'unknown')}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Failed to connect to Ollama: {e}\")\n",
    "    print(\"   Make sure Ollama is running on http://localhost:11434\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1. Model Management and Listing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available Models (using ollama client):\n",
      "   1. gemma3:4b\n",
      "      Size: 3338801804 bytes\n",
      "      Modified: 2025-07-02 21:33:20.686480+05:00\n",
      "      Family: gemma3\n",
      "      Format: gguf\n",
      "      Parameter Size: 4.3B\n",
      "      Quantization: Q4_K_M\n",
      "\n",
      "   2. deepscaler:latest\n",
      "      Size: 3560419491 bytes\n",
      "      Modified: 2025-07-02 21:30:07.716100+05:00\n",
      "      Family: qwen2\n",
      "      Format: gguf\n",
      "      Parameter Size: 1.8B\n",
      "      Quantization: F16\n",
      "\n",
      "   3. gemma3n:e4b\n",
      "      Size: 7547589116 bytes\n",
      "      Modified: 2025-07-02 17:12:43.919932+05:00\n",
      "      Family: gemma3n\n",
      "      Format: gguf\n",
      "      Parameter Size: 6.9B\n",
      "      Quantization: Q4_K_M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all available models using ollama Python client\n",
    "import ollama\n",
    "\n",
    "print(\"üìã Available Models (using ollama client):\")\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    #print(models)\n",
    "    if models and hasattr(models, 'models'):\n",
    "        for i, model in enumerate(models.models, 1):\n",
    "            print(f\"   {i}. {model.model}\")  # Use .model instead of ['name']\n",
    "            print(f\"      Size: {model.size} bytes\")  # Use .size instead of .get('size')\n",
    "            print(f\"      Modified: {model.modified_at}\")  # Use .modified_at instead of .get()\n",
    "            if hasattr(model, 'details'):\n",
    "                details = model.details\n",
    "                print(f\"      Family: {details.family}\")\n",
    "                print(f\"      Format: {details.format}\")\n",
    "                print(f\"      Parameter Size: {details.parameter_size}\")\n",
    "                print(f\"      Quantization: {details.quantization_level}\")\n",
    "            print()\n",
    "    # else:\n",
    "    #     print(\"   No models found\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error listing models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Model Details for deepscaler:\n",
      "   Model: # Modelfile generated by \"ollama show\"\n",
      "# To build a new Modelfile based on this, replace FROM with:\n",
      "# FROM deepscaler:latest\n",
      "\n",
      "FROM C:\\Users\\Jawad\\.ollama\\models\\blobs\\sha256-95ff0bccfe6096c58d176bcbe8d0c87ccc4b517c0eade8acaa0797a9e441122e\n",
      "TEMPLATE \"\"\"{{- if .System }}{{ .System }}{{ end }}\n",
      "{{- range $i, $_ := .Messages }}\n",
      "{{- $last := eq (len (slice $.Messages $i)) 1}}\n",
      "{{- if eq .Role \"user\" }}<ÔΩúUserÔΩú>{{ .Content }}\n",
      "{{- else if eq .Role \"assistant\" }}<ÔΩúAssistantÔΩú>{{ .Content }}{{- if not $last }}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>{{- end }}\n",
      "{{- end }}\n",
      "{{- if and $last (ne .Role \"assistant\") }}<ÔΩúAssistantÔΩú>{{- end }}\n",
      "{{- end }}\"\"\"\n",
      "PARAMETER stop <ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>\n",
      "PARAMETER stop <ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "PARAMETER stop <ÔΩúUserÔΩú>\n",
      "PARAMETER stop <ÔΩúAssistantÔΩú>\n",
      "PARAMETER temperature 0.6\n",
      "PARAMETER top_p 0.95\n",
      "LICENSE \"\"\"MIT License\n",
      "\n",
      "Copyright (c) 2025 Agentica\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\"\"\"\n",
      "\n",
      "   License: MIT License\n",
      "\n",
      "Copyright (c) 2025 Agentica\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n",
      "   Family: qwen2\n",
      "   Format: gguf\n",
      "   Parameters: 1.8B\n",
      "   Quantization: F16\n",
      "\n",
      "   Modelfile (first 10 lines):\n",
      "      # Modelfile generated by \"ollama show\"\n",
      "      # To build a new Modelfile based on this, replace FROM with:\n",
      "      # FROM deepscaler:latest\n",
      "      FROM C:\\Users\\Jawad\\.ollama\\models\\blobs\\sha256-95ff0bccfe6096c58d176bcbe8d0c87ccc4b517c0eade8acaa0797a9e441122e\n",
      "      TEMPLATE \"\"\"{{- if .System }}{{ .System }}{{ end }}\n",
      "      {{- range $i, $_ := .Messages }}\n",
      "      {{- $last := eq (len (slice $.Messages $i)) 1}}\n",
      "      {{- if eq .Role \"user\" }}<ÔΩúUserÔΩú>{{ .Content }}\n",
      "      {{- else if eq .Role \"assistant\" }}<ÔΩúAssistantÔΩú>{{ .Content }}{{- if not $last }}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>{{- end }}\n",
      "      ... (32 more lines)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"deepscaler\"\n",
    "\n",
    "# Get detailed information about our specific model\n",
    "print(f\"üîç Model Details for {MODEL_NAME}:\")\n",
    "try:\n",
    "    model_info = ollama.show(MODEL_NAME)\n",
    "    \n",
    "    print(f\"   Model: {model_info.get('modelfile', 'N/A')}\")\n",
    "    print(f\"   License: {model_info.get('license', 'N/A')}\")\n",
    "    \n",
    "    if 'details' in model_info:\n",
    "        details = model_info['details']\n",
    "        print(f\"   Family: {details.get('family', 'N/A')}\")\n",
    "        print(f\"   Format: {details.get('format', 'N/A')}\")\n",
    "        print(f\"   Parameters: {details.get('parameter_size', 'N/A')}\")\n",
    "        print(f\"   Quantization: {details.get('quantization_level', 'N/A')}\")\n",
    "    \n",
    "    # Show model file contents (truncated)\n",
    "    if 'modelfile' in model_info:\n",
    "        modelfile = model_info['modelfile']\n",
    "        lines = modelfile.split('\\n')[:10]  # First 10 lines\n",
    "        print(f\"\\n   Modelfile (first 10 lines):\")\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                print(f\"      {line}\")\n",
    "        if len(modelfile.split('\\n')) > 10:\n",
    "            print(f\"      ... ({len(modelfile.split('\\n')) - 10} more lines)\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"   Error getting model info: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2. Method 1: Native Ollama Python Client (Non-Streaming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Non-Streaming Generation (ollama.generate)\n",
      "Prompt: Explain quantum entanglement in exactly 2 sentences.\n",
      "\n",
      "Response:\n",
      "<think>\n",
      "Okay, so I need to explain quantum entanglement in exactly two sentences. Let me start by recalling what I know about it. Quantum entanglement is a phenomenon where particles become interconnected such that the state of one particle instantly influences the state of another, no matter how far apart they are.\n",
      "\n",
      "First sentence: Maybe talk about pairs of particles being connected and how their states affect each other immediately. Use terms like \"spooky action at a distance\" to illustrate the mystery or the non-local nature.\n",
      "\n",
      "Second sentence: Need to elaborate on what happens when you measure one particle's state, it affects the other instantly. Emphasize that this is not classical physics but instead relies on quantum mechanics principles like superposition and entanglement.\n",
      "\n",
      "I should make sure both sentences are concise and cover the key aspects without being too technical.\n",
      "</think>\n",
      "\n",
      "Quantum entanglement refers to pairs of particles where their states are interconnected, such that measuring one instantly influences the state of another, regardless of distance. When a particle's state is measured, it affects the other immediately according to quantum mechanics principles.\n",
      "\n",
      "Quantum entanglement involves particles where measurements on one instantaneously affect the other, demonstrating non-local correlations and violating classical physics predictions through superposition and entanglement effects.\n",
      "\n",
      "üïí Wall time: 27.74s\n",
      "\n",
      "‚è±Ô∏è  Non-Streaming Metrics\n",
      "   total_ms: 27707.32\n",
      "   load_ms: 4648.18\n",
      "   prompt_eval_ms: 460.25\n",
      "   eval_ms: 22589.85\n",
      "   prompt_eval_count: 15\n",
      "   eval_count: 256\n",
      "   tokens_per_second: 11.33\n"
     ]
    }
   ],
   "source": [
    "# Non-streaming generation using ollama.generate()\n",
    "prompt = \"Explain quantum entanglement in exactly 2 sentences.\"\n",
    "\n",
    "print(\"üîÑ Non-Streaming Generation (ollama.generate)\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nResponse:\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    response = ollama.generate(model=MODEL_NAME, prompt=prompt)\n",
    "    wall_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"{response['response']}\")\n",
    "    print(f\"\\nüïí Wall time: {wall_time:.2f}s\")\n",
    "    \n",
    "    # Show detailed metrics\n",
    "    show_metrics(response, \"Non-Streaming Metrics\")\n",
    "    \n",
    "    # Store response for later comparison\n",
    "    non_streaming_response = response\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3. Method 2: Native Ollama Python Client (Streaming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä Streaming Generation (ollama.generate)\n",
      "Prompt: List the first 8 prime numbers and explain why 1 is not considered prime.\n",
      "\n",
      "Response:\n",
      "<think>\n",
      "Okay, so I need to list the first 8 prime numbers and explain why 1 isn't considered a prime number. Hmm, let's start by recalling what prime numbers are. Prime numbers are natural numbers greater than 1 that have no positive divisors other than 1 and themselves. So they can't be divided evenly by any other numbers except 1 and themselves.\n",
      "\n",
      "Alright, so the first step is to list out the primes starting from the smallest. Let's begin with the number 2 because it's the smallest prime number. Is 2 a prime? Yes, because its only divisors are 1 and 2. So that's our first prime.\n",
      "\n",
      "Next would be 3. Checking if it's divisible by anything other than 1 and 3. The numbers to check are 2 (since we've already considered 1). Wait, no‚Äîactually, since the next number after 2 is 3, let me think again. After 2 comes 3. To check if 3 is prime, we need to see if it has any divisors other than 1 and itself. The numbers less than sqrt(3) are only up to about 1.7, so checking divisibility by primes less than or equal to that doesn't matter here because the next number after 2 is a prime.\n",
      "\n",
      "But perhaps a better way is just to remember that all primes start from 2 and then proceed in order. So let's list them step by step:\n",
      "\n",
      "1. The first prime number is 2.\n",
      "2. Next, check 3: it doesn't have any divisors other than 1 and 3, so it's prime.\n",
      "3. Then 4: Let's see if 4 is prime. Divided by 2? Yes, because 4/2=2. So it's not prime.\n",
      "4. Next is 5: Check divisibility. It's not divisible by 2 (since it's odd), and the next number to check would be up to sqrt(5) which is around 2.236. Since we've only checked for primes less than that, but since it doesn't divide by any numbers other than 1 and itself, so 5 is prime.\n",
      "5. Then 6: Divisible by 2 and 3, so not prime.\n",
      "6. Next is 7: It's not divisible by 2 or 3 (since 7/2=3.5, which isn't integer; same with 3). The square root of 7 is about 2.645, so again we don't need to check beyond that because if it had any divisors beyond that, they'd pair with numbers less than the square root. So since no divisors found, 7 is prime.\n",
      "7. Next number: 8. Divisible by 2, not prime.\n",
      "8. Then 9: Divisible by 3 (since 9/3=3), so not prime.\n",
      "9. Next is 10: Even number, divisible by 2, not prime.\n",
      "10. Then 11: Let's check if it's prime. It isn't even, doesn't end with 5 except here we're at 11. Check divisibility up to sqrt(11) which is about 3.316. So check for primes less than or equal to that‚Äî2 and 3.\n",
      "\n",
      "Is 11 divisible by 2? No. By 3? Let's see, 1+1=2, which isn't divisible by 3. So yes, 11 is prime.\n",
      "Wait, but actually after 7 comes 8,9,10,11. Wait, perhaps I made an error in the order.\n",
      "\n",
      "Let me correct this step-by-step:\n",
      "\n",
      "Starting from 2:\n",
      "1. 2: Prime\n",
      "2. 3: Prime (since it's not divisible by any number except 1 and itself)\n",
      "3. 4: Not prime (divisible by 2)\n",
      "4. 5: Prime\n",
      "5. 6: Not prime (divisible by 2,3)\n",
      "6. 7: Prime\n",
      "7. 8: Not prime (divisible by 2)\n",
      "8. 9: Not prime (divisible by 3)\n",
      "9. 10: Not prime (divisible by 2,5)\n",
      "10. 11: Now this is where I need to check if it's prime.\n",
      "\n",
      "But perhaps a better approach is just to list the primes in order:\n",
      "\n",
      "The first few primes are known as follows:\n",
      "- The smallest prime is 2.\n",
      "- Next is 3.\n",
      "- Then 5.\n",
      "- Followed by 7.\n",
      "- After that, 11.\n",
      "- Then 13.\n",
      "- Next is 17.\n",
      "- And then 19.\n",
      "\n",
      "Wait, but the user asked for the first 8 primes. So let's list them:\n",
      "\n",
      "1st: 2\n",
      "\n",
      "2nd: 3\n",
      "\n",
      "3rd:5\n",
      "\n",
      "4th:7\n",
      "\n",
      "5th:11\n",
      "\n",
      "6th:13\n",
      "\n",
      "7th:17\n",
      "\n",
      "8th:19.\n",
      "\n",
      "So that would be the first eight prime numbers.\n",
      "\n",
      "Now, why is 1 not considered a prime number? Well, according to the definition of prime numbers, they must have exactly two distinct positive divisors: 1 and themselves. But for 1, its only divisor is itself, which means it has only one positive divisor. Therefore, by this definition, since primes require two distinct divisors (including 1 and themselves), 1 doesn't qualify.\n",
      "\n",
      "Additionally, from the Fundamental Theorem of Arithmetic, every integer greater than 1 can be factored into primes uniquely. If we consider whether 1 is a prime, it would complicate this theorem because multiplying any number by 1 wouldn't change its factors beyond itself. This could lead to multiple factorizations for numbers that are products involving 1 and other primes. Hence, excluding 1 from the list of primes ensures that every integer greater than 1 has a unique prime factorization.\n",
      "\n",
      "So putting it all together: The first eight primes are 2,3,5,7,11,13,17,19, and because 1 only has one divisor, it's not considered prime.\n",
      "</think>\n",
      "\n",
      "The first 8 prime numbers are:\n",
      "\n",
      "2, 3, 5, 7, 11, 13, 17, 19.\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "- **Definition of Prime Numbers:** A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself.\n",
      "  \n",
      "- **Excluding 1:** Since the only divisor of 1 is itself, it does not meet the criteria for being a prime number. This prevents issues in the Fundamental Theorem of Arithmetic when considering unique factorization.\n",
      "\n",
      "Thus, excluding 1 ensures that every integer greater than 1 can be uniquely factored into primes.\n",
      "\n",
      "üïí Wall time: 144.94s\n",
      "üöÄ First token latency: 0.785s\n",
      "‚ö° Average inter-token latency: 0.099s\n",
      "\n",
      "‚è±Ô∏è  Streaming Metrics\n",
      "   total_ms: 144911.49\n",
      "   load_ms: 53.21\n",
      "   prompt_eval_ms: 716.58\n",
      "   eval_ms: 144139.03\n",
      "   prompt_eval_count: 20\n",
      "   eval_count: 1452\n",
      "   tokens_per_second: 10.07\n"
     ]
    }
   ],
   "source": [
    "# Streaming generation using ollama.generate()\n",
    "prompt = \"List the first 8 prime numbers and explain why 1 is not considered prime.\"\n",
    "\n",
    "print(\"üåä Streaming Generation (ollama.generate)\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nResponse:\")\n",
    "\n",
    "start_time = time.time()\n",
    "token_times = []\n",
    "first_token_time = None\n",
    "final_response = None\n",
    "\n",
    "try:\n",
    "    stream = ollama.generate(model=MODEL_NAME, prompt=prompt, stream=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Track first token latency\n",
    "        if first_token_time is None:\n",
    "            first_token_time = current_time - start_time\n",
    "        \n",
    "        # Track inter-token latency\n",
    "        if len(token_times) > 0:\n",
    "            inter_token_latency = current_time - token_times[-1]\n",
    "            token_times.append(current_time)\n",
    "        else:\n",
    "            token_times.append(current_time)\n",
    "        \n",
    "        # Print the token\n",
    "        print(chunk['response'], end='', flush=True)\n",
    "        \n",
    "        # Keep reference to final chunk for metrics\n",
    "        final_response = chunk\n",
    "    \n",
    "    wall_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\nüïí Wall time: {wall_time:.2f}s\")\n",
    "    print(f\"üöÄ First token latency: {first_token_time:.3f}s\")\n",
    "    \n",
    "    # Calculate average inter-token latency\n",
    "    if len(token_times) > 1:\n",
    "        inter_token_latencies = [token_times[i] - token_times[i-1] for i in range(1, len(token_times))]\n",
    "        avg_inter_token = sum(inter_token_latencies) / len(inter_token_latencies)\n",
    "        print(f\"‚ö° Average inter-token latency: {avg_inter_token:.3f}s\")\n",
    "    \n",
    "    # Show detailed metrics\n",
    "    if final_response:\n",
    "        show_metrics(final_response, \"Streaming Metrics\")\n",
    "        streaming_response = final_response\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4. Method 3: Chat API (Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat API Streaming (ollama.chat)\n",
      "Messages:\n",
      "  system: You are a helpful assistant that provides concise, accurate answers.\n",
      "  user: What are the key differences between Python lists and tuples? Give 3 main points.\n",
      "\n",
      "Response:\n",
      "<think>\n",
      "Okay, I need to explain the key differences between Python lists and tuples. The user wants three main points.\n",
      "\n",
      "First, maybe list is ordered but can have duplicates because lists allow item insertion anywhere. Wait, no, actually lists don't support duplicate values unless you add them again, which isn't possible directly. Hmm, perhaps that's not a good point.\n",
      "\n",
      "Wait, the user asked for differences between lists and tuples. Tuples are immutable. So maybe the first point is about mutability. That makes sense because tuples can't be changed after creation.\n",
      "\n",
      "Second, another difference might be elements in a list have pointers to their positions, but since they're mutable, you can change values without issue. Wait, no, that's not right. Actually, lists are dynamic; you can add or remove elements anywhere. Tuples are fixed size and immutable, so once created, you can't change any elements.\n",
      "\n",
      "Third point could be about the structure. Lists contain pointers to their own internal storage (like a list of integers), while tuples don't have such overhead because they're just a collection of items. So perhaps that's another key difference: lists are mutable and can grow larger, whereas tuples are fixed size and immutable.\n",
      "\n",
      "Wait, maybe I need to think again. Let me outline:\n",
      "\n",
      "1. Mutable vs Immutable: Tuples are immutable; you can't change them after creation. Lists are mutable since elements can be added or removed anywhere.\n",
      "\n",
      "2. Size: Tuples have a fixed size specified when created, while lists grow in length as needed without bounds.\n",
      "\n",
      "3. Elements: Since lists allow duplicates (as long as they're not negative and within range), perhaps that's another point. Wait no, because lists don't support duplicate values unless you use specific functions like insert or append multiple times. But maybe it's better to focus on the first three points I thought of.\n",
      "</think>\n",
      "\n",
      "The key differences between Python lists and tuples are:\n",
      "\n",
      "1. **Mutability**: Tuples are immutable; once created, their contents cannot be changed. Lists are mutable; elements can be added, removed, or modified as needed.\n",
      "\n",
      "2. **Size**: Tuples have a fixed size specified when created, while lists grow in length dynamically without bounds.\n",
      "\n",
      "3. **Element Density and Growth**: Lists allow for dynamic growth (adding elements at any position), whereas tuples have a fixed structure with no room for expansion.\n",
      "\n",
      "These three points highlight the primary distinctions between lists and tuples in Python.\n",
      "\n",
      "üïí Wall time: 45.99s\n",
      "\n",
      "‚è±Ô∏è  Chat API Streaming Metrics\n",
      "   total_ms: 45908.22\n",
      "   load_ms: 47.61\n",
      "   prompt_eval_ms: 756.59\n",
      "   eval_ms: 45101.8\n",
      "   prompt_eval_count: 32\n",
      "   eval_count: 498\n",
      "   tokens_per_second: 11.04\n"
     ]
    }
   ],
   "source": [
    "# Chat API with streaming\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise, accurate answers.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the key differences between Python lists and tuples? Give 3 main points.\"}\n",
    "]\n",
    "\n",
    "print(\"üí¨ Chat API Streaming (ollama.chat)\")\n",
    "print(\"Messages:\")\n",
    "for msg in messages:\n",
    "    print(f\"  {msg['role']}: {msg['content']}\")\n",
    "print(\"\\nResponse:\")\n",
    "\n",
    "start_time = time.time()\n",
    "final_chunk = None\n",
    "\n",
    "try:\n",
    "    stream = ollama.chat(model=MODEL_NAME, messages=messages, stream=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        # Print the content from the message\n",
    "        content = chunk['message']['content']\n",
    "        print(content, end='', flush=True)\n",
    "        final_chunk = chunk\n",
    "    \n",
    "    wall_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\nüïí Wall time: {wall_time:.2f}s\")\n",
    "    \n",
    "    # Show detailed metrics\n",
    "    if final_chunk:\n",
    "        show_metrics(final_chunk, \"Chat API Streaming Metrics\")\n",
    "        chat_response = final_chunk\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 5. Method 4: Raw REST API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Raw REST API Streaming\n",
      "Endpoint: http://localhost:11434/api/generate\n",
      "Payload: {\n",
      "  \"model\": \"deepscaler\",\n",
      "  \"prompt\": \"Explain the concept of recursion in programming with a simple example.\",\n",
      "  \"stream\": true\n",
      "}\n",
      "\n",
      "Response:\n",
      "<think>\n",
      "Okay, so I need to explain recursion in programming with a simple example. Hmm, let's see. Recursion is when a function calls itself repeatedly until a base case is reached. That makes sense.\n",
      "\n",
      "I remember that with math problems like calculating factorials or Fibonacci numbers, recursion is often used because it breaks down the problem into smaller parts which are easier to solve.\n",
      "\n",
      "Let me think of an example. Maybe the factorial function. The factorial of a number n is n multiplied by (n-1) factorial until we reach 1. So for example, 5! = 5*4!, and so on.\n",
      "\n",
      "I should write down how this works step by step. Start with a function that takes n as an argument. If n is 0 or 1, return 1 because that's the base case. Otherwise, multiply n by factorial(n-1). That way, each call reduces the problem size until it hits the base case.\n",
      "\n",
      "Let me try writing this in code to make it clearer. The function would check if n <= 1, then return 1. Else, return n * factorial(n-1).\n",
      "\n",
      "I should also mention why recursion is useful here‚Äîbecause it's concise and avoids loops when possible. It makes the code easier to understand for someone who might not be familiar with iteration.\n",
      "\n",
      "Wait, maybe I can think of another example, like using recursion in a loop or something else. But perhaps that's beyond the scope since the user asked for a simple explanation.\n",
      "\n",
      "I should make sure the example is clear and shows how each step works without getting too complicated. Maybe start with small numbers to illustrate the process.\n",
      "</think>\n",
      "\n",
      "Recursion is a programming concept where a function solves a problem by breaking it down into smaller, similar subproblems. Each of these subproblems is solved in the same way until they reach a base case, which is a solution that doesn't require further division.\n",
      "\n",
      "**Example: Calculating Factorial Using Recursion**\n",
      "\n",
      "Let's define a factorial function `fact(n)` where:\n",
      "\n",
      "- **Base Case:** If n <= 1, return 1.\n",
      "- **Recursive Step:** Multiply n by the result of calling `fact(n - 1)`.\n",
      "\n",
      "For example:\n",
      "- `fact(5) = 5 * fact(4)`\n",
      "- `fact(4) = 4 * fact(3)`\n",
      "- ...\n",
      "- This continues until it reaches `fact(1)`, which returns 1.\n",
      "\n",
      "Thus, `fact(5) = 5*4*3*2*1 = 120`.\n",
      "\n",
      "**Why is recursion useful?**\n",
      "It allows for concise and readable code by leveraging the function's own structure, making it easier to understand complex problems through simple recursive calls.\n",
      "\n",
      "üïí Wall time: 55.59s\n",
      "\n",
      "‚è±Ô∏è  Raw REST API Metrics\n",
      "   total_ms: 53540.45\n",
      "   load_ms: 50.36\n",
      "   prompt_eval_ms: 713.91\n",
      "   eval_ms: 52775.5\n",
      "   prompt_eval_count: 16\n",
      "   eval_count: 554\n",
      "   tokens_per_second: 10.5\n"
     ]
    }
   ],
   "source": [
    "# Raw REST API streaming\n",
    "payload = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"prompt\": \"Explain the concept of recursion in programming with a simple example.\",\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "print(\"üîó Raw REST API Streaming\")\n",
    "print(f\"Endpoint: {BASE_URL}/api/generate\")\n",
    "print(f\"Payload: {json.dumps(payload, indent=2)}\")\n",
    "print(\"\\nResponse:\")\n",
    "\n",
    "start_time = time.time()\n",
    "final_data = None\n",
    "\n",
    "try:\n",
    "    with requests.post(f\"{BASE_URL}/api/generate\", json=payload, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if not line:  # Skip empty lines (keep-alive)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                token = data.get('response', '')\n",
    "                print(token, end='', flush=True)\n",
    "                \n",
    "                if data.get('done', False):\n",
    "                    final_data = data\n",
    "                    break\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    wall_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\nüïí Wall time: {wall_time:.2f}s\")\n",
    "    \n",
    "    # Show detailed metrics\n",
    "    if final_data:\n",
    "        show_metrics(final_data, \"Raw REST API Metrics\")\n",
    "        rest_response = final_data\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Method 5: LangChain Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain integration with streaming\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "print(\"ü¶ú LangChain Integration (Synchronous Streaming)\")\n",
    "\n",
    "# Create LangChain Ollama instance\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME,\n",
    "    base_url=BASE_URL,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "prompt = \"Describe the water cycle in nature using exactly 4 steps.\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nResponse:\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Streaming with LangChain\n",
    "    response_chunks = []\n",
    "    for chunk in llm.stream(prompt):\n",
    "        print(chunk, end='', flush=True)\n",
    "        response_chunks.append(chunk)\n",
    "    \n",
    "    wall_time = time.time() - start_time\n",
    "    full_response = ''.join(response_chunks)\n",
    "    \n",
    "    print(f\"\\n\\nüïí Wall time: {wall_time:.2f}s\")\n",
    "    print(f\"üìù Response length: {len(full_response)} characters\")\n",
    "    print(f\"üßÆ Estimated tokens: ~{len(full_response.split())} words\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Async streaming\n",
    "import asyncio\n",
    "\n",
    "async def async_langchain_streaming():\n",
    "    print(\"ü¶úüöÄ LangChain Integration (Asynchronous Streaming)\")\n",
    "    \n",
    "    prompt = \"What are the main components of a computer? List 5 key parts.\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response_chunks = []\n",
    "        async for chunk in llm.astream(prompt):\n",
    "            print(chunk, end='', flush=True)\n",
    "            response_chunks.append(chunk)\n",
    "        \n",
    "        wall_time = time.time() - start_time\n",
    "        full_response = ''.join(response_chunks)\n",
    "        \n",
    "        print(f\"\\n\\nüïí Wall time: {wall_time:.2f}s\")\n",
    "        print(f\"üìù Response length: {len(full_response)} characters\")\n",
    "        print(f\"üßÆ Estimated tokens: ~{len(full_response.split())} words\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Run the async function\n",
    "await async_langchain_streaming()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Embeddings Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embeddings (if the model supports them)\n",
    "print(\"üî¢ Testing Embeddings\")\n",
    "\n",
    "# Note: Not all models support embeddings. We'll try with the current model first,\n",
    "# then fall back to a dedicated embedding model if available\n",
    "\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Python is a popular programming language.\"\n",
    "]\n",
    "\n",
    "print(\"Test texts:\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"  {i}. {text}\")\n",
    "\n",
    "try:\n",
    "    print(f\"\\nTrying embeddings with {MODEL_NAME}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Try single embedding first\n",
    "    embedding_result = ollama.embed(model=MODEL_NAME, input=test_texts[0])\n",
    "    \n",
    "    if 'embeddings' in embedding_result:\n",
    "        embedding = embedding_result['embeddings'][0]\n",
    "        print(f\"‚úÖ Single embedding successful!\")\n",
    "        print(f\"   Embedding dimension: {len(embedding)}\")\n",
    "        print(f\"   First 10 values: {embedding[:10]}\")\n",
    "        \n",
    "        # Try batch embeddings\n",
    "        batch_result = ollama.embed(model=MODEL_NAME, input=test_texts)\n",
    "        batch_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n‚úÖ Batch embeddings successful!\")\n",
    "        print(f\"   Number of embeddings: {len(batch_result['embeddings'])}\")\n",
    "        print(f\"   Time taken: {batch_time:.3f}s\")\n",
    "        \n",
    "        # Calculate similarities (dot product)\n",
    "        import numpy as np\n",
    "        embeddings = np.array(batch_result['embeddings'])\n",
    "        \n",
    "        print(f\"\\nüìä Similarity matrix (dot product):\")\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(len(embeddings)):\n",
    "                similarity = np.dot(embeddings[i], embeddings[j])\n",
    "                print(f\"  Text {i+1} ‚Üî Text {j+1}: {similarity:.4f}\")\n",
    "    else:\n",
    "        print(f\"‚ùå No embeddings in response\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Embeddings failed with {MODEL_NAME}: {e}\")\n",
    "    \n",
    "    # Try with a common embedding model if available\n",
    "    embedding_models = [\"nomic-embed-text\", \"all-minilm\", \"mxbai-embed-large\"]\n",
    "    \n",
    "    for embed_model in embedding_models:\n",
    "        try:\n",
    "            print(f\"\\nTrying with {embed_model}...\")\n",
    "            test_result = ollama.embed(model=embed_model, input=test_texts[0])\n",
    "            print(f\"‚úÖ {embed_model} works! Use this model for embeddings.\")\n",
    "            break\n",
    "        except Exception as embed_e:\n",
    "            print(f\"‚ùå {embed_model} not available: {embed_e}\")\n",
    "    else:\n",
    "        print(\"\\nüí° To test embeddings, pull an embedding model like:\")\n",
    "        print(\"   ollama pull nomic-embed-text\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Performance Comparison and Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking across different methods\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "def benchmark_method(method_name, method_func, iterations=3):\n",
    "    \"\"\"Benchmark a specific method multiple times\"\"\"\n",
    "    print(f\"\\nüèÅ Benchmarking {method_name} ({iterations} iterations)\")\n",
    "    \n",
    "    times = []\n",
    "    token_counts = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print(f\"  Run {i+1}/{iterations}...\", end=\" \")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = method_func()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            duration = end_time - start_time\n",
    "            times.append(duration)\n",
    "            \n",
    "            # Extract token count if available\n",
    "            if isinstance(result, dict):\n",
    "                tokens = result.get('eval_count', result.get('completion_tokens', 0))\n",
    "                token_counts.append(tokens)\n",
    "            \n",
    "            print(f\"{duration:.2f}s ({tokens if 'tokens' in locals() else '?'} tokens)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if times:\n",
    "        avg_time = statistics.mean(times)\n",
    "        std_time = statistics.stdev(times) if len(times) > 1 else 0\n",
    "        min_time = min(times)\n",
    "        max_time = max(times)\n",
    "        \n",
    "        print(f\"  üìä Results:\")\n",
    "        print(f\"     Average: {avg_time:.3f}s ¬± {std_time:.3f}s\")\n",
    "        print(f\"     Range: {min_time:.3f}s - {max_time:.3f}s\")\n",
    "        \n",
    "        if token_counts:\n",
    "            avg_tokens = statistics.mean(token_counts)\n",
    "            avg_tokens_per_sec = avg_tokens / avg_time if avg_time > 0 else 0\n",
    "            print(f\"     Avg tokens: {avg_tokens:.1f}\")\n",
    "            print(f\"     Tokens/sec: {avg_tokens_per_sec:.1f}\")\n",
    "        \n",
    "        return {\n",
    "            'method': method_name,\n",
    "            'avg_time': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'min_time': min_time,\n",
    "            'max_time': max_time,\n",
    "            'avg_tokens': statistics.mean(token_counts) if token_counts else None,\n",
    "            'tokens_per_sec': avg_tokens_per_sec if token_counts else None\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Define test prompt\n",
    "test_prompt = \"Explain the difference between RAM and storage in computers. Be concise.\"\n",
    "\n",
    "# Define benchmark methods\n",
    "def bench_generate_nonstreaming():\n",
    "    return ollama.generate(model=MODEL_NAME, prompt=test_prompt)\n",
    "\n",
    "def bench_generate_streaming():\n",
    "    final_chunk = None\n",
    "    for chunk in ollama.generate(model=MODEL_NAME, prompt=test_prompt, stream=True):\n",
    "        final_chunk = chunk\n",
    "    return final_chunk\n",
    "\n",
    "def bench_chat_streaming():\n",
    "    messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "    final_chunk = None\n",
    "    for chunk in ollama.chat(model=MODEL_NAME, messages=messages, stream=True):\n",
    "        final_chunk = chunk\n",
    "    return final_chunk\n",
    "\n",
    "def bench_rest_api():\n",
    "    payload = {\"model\": MODEL_NAME, \"prompt\": test_prompt, \"stream\": False}\n",
    "    response = requests.post(f\"{BASE_URL}/api/generate\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Run benchmarks\n",
    "print(f\"üöÄ Performance Benchmarking\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Test prompt: {test_prompt}\")\n",
    "\n",
    "benchmarks = []\n",
    "\n",
    "# Benchmark each method\n",
    "methods = [\n",
    "    (\"Generate (Non-streaming)\", bench_generate_nonstreaming),\n",
    "    (\"Generate (Streaming)\", bench_generate_streaming), \n",
    "    (\"Chat (Streaming)\", bench_chat_streaming),\n",
    "    (\"REST API (Non-streaming)\", bench_rest_api)\n",
    "]\n",
    "\n",
    "for method_name, method_func in methods:\n",
    "    result = benchmark_method(method_name, method_func, iterations=3)\n",
    "    if result:\n",
    "        benchmarks.append(result)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "if benchmarks:\n",
    "    print(f\"\\nüìà Performance Comparison Summary\")\n",
    "    df = pd.DataFrame(benchmarks)\n",
    "    df = df.round(3)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Find fastest method\n",
    "    fastest = df.loc[df['avg_time'].idxmin()]\n",
    "    print(f\"\\nüèÜ Fastest method: {fastest['method']} ({fastest['avg_time']:.3f}s avg)\")\n",
    "    \n",
    "    if 'tokens_per_sec' in df.columns and df['tokens_per_sec'].notna().any():\n",
    "        highest_throughput = df.loc[df['tokens_per_sec'].idxmax()]\n",
    "        print(f\"‚ö° Highest throughput: {highest_throughput['method']} ({highest_throughput['tokens_per_sec']:.1f} tokens/sec)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9. Advanced Features Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced features like custom options, context, etc.\n",
    "print(\"üß™ Advanced Features Testing\")\n",
    "\n",
    "# Test 1: Custom generation options\n",
    "print(\"\\n1Ô∏è‚É£ Custom Generation Options\")\n",
    "custom_options = {\n",
    "    \"temperature\": 0.1,  # More deterministic\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 40,\n",
    "    \"repeat_penalty\": 1.1,\n",
    "    \"num_predict\": 50  # Limit response length\n",
    "}\n",
    "\n",
    "prompt = \"Write a creative story opening.\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Options: {custom_options}\")\n",
    "print(\"\\nResponse:\")\n",
    "\n",
    "try:\n",
    "    response = ollama.generate(\n",
    "        model=MODEL_NAME, \n",
    "        prompt=prompt,\n",
    "        options=custom_options\n",
    "    )\n",
    "    print(response['response'])\n",
    "    show_metrics(response, \"Custom Options Metrics\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test 2: Context/conversation memory\n",
    "print(\"\\n\\n2Ô∏è‚É£ Context and Conversation Memory\")\n",
    "print(\"Testing conversation continuity...\")\n",
    "\n",
    "# First message\n",
    "context = None\n",
    "messages = []\n",
    "\n",
    "try:\n",
    "    # First turn\n",
    "    print(\"\\nTurn 1:\")\n",
    "    user_msg1 = \"My name is Alice and I love astronomy.\"\n",
    "    print(f\"User: {user_msg1}\")\n",
    "    \n",
    "    response1 = ollama.generate(model=MODEL_NAME, prompt=user_msg1)\n",
    "    context = response1.get('context')  # Save context for next turn\n",
    "    \n",
    "    print(f\"Assistant: {response1['response']}\")\n",
    "    \n",
    "    # Second turn using context\n",
    "    print(\"\\nTurn 2:\")\n",
    "    user_msg2 = \"What do you remember about me?\"\n",
    "    print(f\"User: {user_msg2}\")\n",
    "    \n",
    "    response2 = ollama.generate(\n",
    "        model=MODEL_NAME, \n",
    "        prompt=user_msg2,\n",
    "        context=context  # Use saved context\n",
    "    )\n",
    "    \n",
    "    print(f\"Assistant: {response2['response']}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Context size: {len(context) if context else 0} tokens\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in context test: {e}\")\n",
    "\n",
    "# Test 3: System prompt\n",
    "print(\"\\n\\n3Ô∏è‚É£ System Prompt Testing\")\n",
    "system_prompt = \"You are a helpful assistant who always responds in exactly one sentence and ends with an emoji.\"\n",
    "user_prompt = \"Explain photosynthesis.\"\n",
    "\n",
    "print(f\"System: {system_prompt}\")\n",
    "print(f\"User: {user_prompt}\")\n",
    "print(\"\\nResponse:\")\n",
    "\n",
    "try:\n",
    "    # Using chat API with system message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = ollama.chat(model=MODEL_NAME, messages=messages)\n",
    "    print(response['message']['content'])\n",
    "    show_metrics(response, \"System Prompt Metrics\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test 4: Raw mode (if supported)\n",
    "print(\"\\n\\n4Ô∏è‚É£ Raw Mode Testing\")\n",
    "print(\"Testing raw mode (bypassing template)...\")\n",
    "\n",
    "try:\n",
    "    # Raw mode bypasses the model's chat template\n",
    "    raw_response = ollama.generate(\n",
    "        model=MODEL_NAME,\n",
    "        prompt=\"<|user|>What is 2+2?<|assistant|>\",\n",
    "        raw=True\n",
    "    )\n",
    "    print(f\"Raw response: {raw_response['response']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Raw mode not supported or error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Advanced features testing complete!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 10. Summary and Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all testing results and recommendations\n",
    "print(\"üìã OLLAMA TESTING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüîß Configuration Used:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Ollama Host: {BASE_URL}\")\n",
    "\n",
    "print(f\"\\nüìä Methods Tested:\")\n",
    "methods_tested = [\n",
    "    \"‚úÖ Native Ollama Python Client (Non-streaming)\",\n",
    "    \"‚úÖ Native Ollama Python Client (Streaming)\", \n",
    "    \"‚úÖ Chat API (Streaming)\",\n",
    "    \"‚úÖ Raw REST API (Streaming)\",\n",
    "    \"‚úÖ LangChain Integration (Sync & Async)\",\n",
    "    \"‚úÖ Embeddings (if supported)\",\n",
    "    \"‚úÖ Performance Benchmarking\",\n",
    "    \"‚úÖ Advanced Features (Custom options, Context, System prompts)\"\n",
    "]\n",
    "\n",
    "for method in methods_tested:\n",
    "    print(f\"   {method}\")\n",
    "\n",
    "print(f\"\\nüí° Key Findings:\")\n",
    "print(f\"   ‚Ä¢ Streaming provides real-time token delivery\")\n",
    "print(f\"   ‚Ä¢ All methods provide detailed metrics (token counts, timing)\")\n",
    "print(f\"   ‚Ä¢ LangChain offers higher-level abstractions\")\n",
    "print(f\"   ‚Ä¢ Raw REST API provides maximum control\")\n",
    "print(f\"   ‚Ä¢ Context/memory enables multi-turn conversations\")\n",
    "print(f\"   ‚Ä¢ Custom options allow fine-tuning model behavior\")\n",
    "\n",
    "print(f\"\\nüöÄ Recommendations for Your App:\")\n",
    "print(f\"   1. Use STREAMING for better user experience\")\n",
    "print(f\"   2. Native ollama client for simplicity\")\n",
    "print(f\"   3. LangChain for complex pipelines\")\n",
    "print(f\"   4. Monitor tokens/sec for performance\")\n",
    "print(f\"   5. Implement proper error handling\")\n",
    "print(f\"   6. Cache context for conversations\")\n",
    "\n",
    "print(f\"\\nüìà Metrics to Track:\")\n",
    "metrics_to_track = [\n",
    "    \"‚Ä¢ prompt_eval_count (input tokens)\",\n",
    "    \"‚Ä¢ eval_count (output tokens)\", \n",
    "    \"‚Ä¢ total_duration (total time)\",\n",
    "    \"‚Ä¢ eval_duration (generation time)\",\n",
    "    \"‚Ä¢ tokens_per_second (throughput)\",\n",
    "    \"‚Ä¢ first_token_latency (responsiveness)\",\n",
    "    \"‚Ä¢ inter_token_latency (smoothness)\"\n",
    "]\n",
    "\n",
    "for metric in metrics_to_track:\n",
    "    print(f\"   {metric}\")\n",
    "\n",
    "print(f\"\\nüõ†Ô∏è Next Steps:\")\n",
    "next_steps = [\n",
    "    \"1. Integrate streaming into your studio app\",\n",
    "    \"2. Add token counting for billing/limits\",\n",
    "    \"3. Implement conversation memory\",\n",
    "    \"4. Add performance monitoring\",\n",
    "    \"5. Test with different models\",\n",
    "    \"6. Add error handling and retries\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(f\"\\n‚ú® All tests completed successfully!\")\n",
    "print(\"=\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
